{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ ANMI 2.0: Adaptive Negative Mining Intelligence\n",
        "\n",
        "## A Complete Implementation for Training Dense Retrievers with ELO-Calibrated Hard Negatives\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
        "\n",
        "---\n",
        "\n",
        "### What This Notebook Covers\n",
        "\n",
        "This notebook implements the **ANMI 2.0 (Adaptive Negative Mining Intelligence)** framework, which synthesizes:\n",
        "\n",
        "1. **Contrastive Learning Theory** - InfoNCE loss with temperature scaling\n",
        "2. **Hard Negative Mining** - Finding challenging negatives for better training\n",
        "3. **ELO-Based Calibration** - Using pairwise comparisons to estimate document quality\n",
        "4. **Hybrid Loss** - Combining contrastive and regression objectives\n",
        "\n",
        "### The Core Problem: The Laffer Curve of Negative Mining\n",
        "\n",
        "```\n",
        "Performance\n",
        "    ‚îÇ\n",
        "    ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ Sweet Spot\n",
        "    ‚îÇ          /‚îÇ\\\n",
        "    ‚îÇ         / ‚îÇ \\        ‚Üê Laffer Curve\n",
        "    ‚îÇ        /  ‚îÇ  \\\n",
        "    ‚îÇ       /   ‚îÇ   \\\n",
        "    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ/‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ\\‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    ‚îÇ  Random   ‚îÇ  Too Hard\n",
        "    ‚îÇ  (boring) ‚îÇ  (false negatives!)\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Mining Difficulty\n",
        "```\n",
        "\n",
        "**Key Insight**: The hardest negatives provide the most gradient signal, BUT they're also most likely to be **false negatives** (actually relevant documents mislabeled as negative). ANMI 2.0 uses ELO scores to find the \"Goldilocks zone\" of difficulty.\n",
        "\n",
        "---\n",
        "\n",
        "**Author**: ANMI Research Team  \n",
        "**Last Updated**: December 2024  \n",
        "**Runtime**: ~20-30 minutes on Colab T4 GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation\n",
        "\n",
        "First, let's install the required packages and check our GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üì¶ INSTALLATION\n",
        "# ============================================================\n",
        "# Install required packages (takes ~2 minutes)\n",
        "\n",
        "%pip install -q sentence-transformers datasets transformers torch numpy scipy tqdm\n",
        "%pip install -q rank_bm25  # For BM25 baseline\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! This notebook will be slow on CPU.\")\n",
        "    print(\"   Go to Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üìö IMPORTS\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from datasets import load_dataset\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset: SciFact\n",
        "\n",
        "We'll use the **SciFact** dataset from BEIR - it's small enough to run on Colab's free tier while still being challenging (scientific claims with high false negative rates ~20%).\n",
        "\n",
        "| Dataset | Queries | Corpus | Domain | Why We Use It |\n",
        "|---------|---------|--------|--------|---------------|\n",
        "| SciFact | 300 (test) | 5,183 docs | Scientific | Small, challenging, high FN rate |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üìä LOAD SCIFACT DATASET\n",
        "# ============================================================\n",
        "\n",
        "print(\"üì• Loading SciFact dataset from HuggingFace...\")\n",
        "\n",
        "# Load corpus (documents)\n",
        "corpus_dataset = load_dataset(\"BeIR/scifact\", \"corpus\", split=\"corpus\")\n",
        "print(f\"   Corpus: {len(corpus_dataset)} documents\")\n",
        "\n",
        "# Load queries\n",
        "queries_dataset = load_dataset(\"BeIR/scifact\", \"queries\", split=\"queries\")\n",
        "print(f\"   Queries: {len(queries_dataset)} queries\")\n",
        "\n",
        "# Load relevance judgments (qrels)\n",
        "# Note: SciFact has train/test splits - we'll use a portion for training\n",
        "qrels_dataset = load_dataset(\"BeIR/scifact-qrels\", split=\"train\")\n",
        "print(f\"   Relevance judgments: {len(qrels_dataset)} pairs\")\n",
        "\n",
        "# Build lookup dictionaries\n",
        "corpus = {str(doc[\"_id\"]): doc[\"text\"] for doc in corpus_dataset}\n",
        "queries = {str(q[\"_id\"]): q[\"text\"] for q in queries_dataset}\n",
        "\n",
        "# Build positive mapping: query_id -> list of relevant doc_ids\n",
        "positives = defaultdict(list)\n",
        "for qrel in qrels_dataset:\n",
        "    if qrel[\"score\"] > 0:  # Only positive relevance\n",
        "        positives[str(qrel[\"query-id\"])].append(str(qrel[\"corpus-id\"]))\n",
        "\n",
        "# Filter to queries that have positives\n",
        "query_ids = [qid for qid in positives.keys() if qid in queries]\n",
        "print(f\"\\n‚úÖ Loaded {len(query_ids)} queries with positive labels\")\n",
        "print(f\"   Average positives per query: {np.mean([len(positives[q]) for q in query_ids]):.1f}\")\n",
        "\n",
        "# Show example\n",
        "example_qid = query_ids[0]\n",
        "print(f\"\\nüìù Example Query (ID: {example_qid}):\")\n",
        "print(f\"   Query: {queries[example_qid][:100]}...\")\n",
        "print(f\"   Positives: {len(positives[example_qid])} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Core ANMI Components\n",
        "\n",
        "### 3.1 The ELO Engine\n",
        "\n",
        "The heart of ANMI is estimating **absolute quality scores** from **pairwise comparisons**.\n",
        "\n",
        "**Why ELO?**\n",
        "- Pairwise judgments (\"Is doc A better than doc B for this query?\") are more reliable than absolute judgments (\"Is doc A relevant?\")\n",
        "- ELO/Thurstone models convert O(n) pairwise comparisons into absolute scores\n",
        "- This lets us find the \"Goldilocks zone\" of difficulty\n",
        "\n",
        "**The Math (Thurstone Model):**\n",
        "```\n",
        "P(doc_i > doc_j | query) = Œ¶((e_i - e_j) / œÉ‚àö2)\n",
        "```\n",
        "Where:\n",
        "- `e_i` = ELO score of document i\n",
        "- `Œ¶` = Standard normal CDF\n",
        "- Higher ELO = more relevant to query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üéØ ELO ENGINE: Sparse ELO Estimation\n",
        "# ============================================================\n",
        "\n",
        "class SparseELOEstimator:\n",
        "    \"\"\"\n",
        "    Estimates ELO scores from sparse pairwise comparisons using Thurstone model.\n",
        "    \n",
        "    Key Innovation: Instead of comparing all O(n¬≤) pairs, we only compare O(n*k) pairs\n",
        "    using a k-regular graph structure. This is mathematically sound because:\n",
        "    - k-regular graphs are connected (ELO differences are well-defined)\n",
        "    - Diameter is O(log n) so estimation error propagation is bounded\n",
        "    \n",
        "    Args:\n",
        "        comparison_degree: Number of comparisons per document (k in k-regular graph)\n",
        "        max_iterations: Maximum gradient ascent iterations for MLE\n",
        "        tolerance: Convergence threshold\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        comparison_degree: int = 4,  # Lower for Colab speed\n",
        "        max_iterations: int = 50,\n",
        "        tolerance: float = 1e-3,\n",
        "    ):\n",
        "        self.k = comparison_degree\n",
        "        self.max_iter = max_iterations\n",
        "        self.tol = tolerance\n",
        "    \n",
        "    def _build_k_regular_graph(self, n: int) -> List[Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Build k-regular graph by unioning k/2 random Hamiltonian cycles.\n",
        "        \n",
        "        A Hamiltonian cycle visits every node exactly once, forming a closed loop.\n",
        "        Unioning multiple such cycles creates a k-regular graph where every node\n",
        "        has exactly k neighbors.\n",
        "        \n",
        "        Example for n=5, k=4 (2 cycles):\n",
        "            Cycle 1: 0 ‚Üí 3 ‚Üí 1 ‚Üí 4 ‚Üí 2 ‚Üí 0\n",
        "            Cycle 2: 0 ‚Üí 2 ‚Üí 4 ‚Üí 1 ‚Üí 3 ‚Üí 0\n",
        "            Combined: Each node has 4 edges\n",
        "        \"\"\"\n",
        "        edges = set()\n",
        "        num_cycles = max(1, self.k // 2)\n",
        "        \n",
        "        for _ in range(num_cycles):\n",
        "            # Generate random permutation (Hamiltonian cycle)\n",
        "            perm = np.random.permutation(n).tolist()\n",
        "            \n",
        "            # Add cycle edges: perm[0]‚Üíperm[1]‚Üí...‚Üíperm[n-1]‚Üíperm[0]\n",
        "            for i in range(n):\n",
        "                edge = tuple(sorted([perm[i], perm[(i + 1) % n]]))\n",
        "                edges.add(edge)\n",
        "        \n",
        "        return list(edges)\n",
        "    \n",
        "    def _fit_thurstone(\n",
        "        self,\n",
        "        preferences: Dict[Tuple[int, int], float],\n",
        "        edges: List[Tuple[int, int]],\n",
        "        n: int,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Fit Thurstone model via gradient ascent on log-likelihood.\n",
        "        \n",
        "        The Thurstone model assumes observed preferences come from comparing\n",
        "        noisy estimates of true quality. If doc i has quality e_i and noise Œµ_i ~ N(0,œÉ¬≤):\n",
        "            P(i > j) = P(e_i + Œµ_i > e_j + Œµ_j) = Œ¶((e_i - e_j) / (œÉ‚àö2))\n",
        "        \n",
        "        We maximize log-likelihood:\n",
        "            ‚Ñì(e) = Œ£ [w_ij * log(Œ¶(e_i - e_j)) + (1-w_ij) * log(Œ¶(e_j - e_i))]\n",
        "        \"\"\"\n",
        "        e = np.zeros(n)  # Initialize ELO scores to zero\n",
        "        \n",
        "        for iteration in range(self.max_iter):\n",
        "            grad = np.zeros(n)\n",
        "            \n",
        "            for (i, j) in edges:\n",
        "                w_ij = preferences.get((i, j), 0.5)\n",
        "                delta = e[i] - e[j]\n",
        "                \n",
        "                # Compute gradient using inverse Mills ratio\n",
        "                # The inverse Mills ratio Œª(x) = œÜ(x)/Œ¶(x) appears naturally\n",
        "                # in the gradient of the Thurstone log-likelihood\n",
        "                phi_delta = norm.pdf(delta)\n",
        "                Phi_delta = norm.cdf(delta)\n",
        "                Phi_neg_delta = 1 - Phi_delta\n",
        "                \n",
        "                # Avoid division by zero\n",
        "                lambda_pos = phi_delta / max(Phi_delta, 1e-10)\n",
        "                lambda_neg = phi_delta / max(Phi_neg_delta, 1e-10)\n",
        "                \n",
        "                grad[i] += w_ij * lambda_pos - (1 - w_ij) * lambda_neg\n",
        "                grad[j] += -w_ij * lambda_pos + (1 - w_ij) * lambda_neg\n",
        "            \n",
        "            # Project onto constraint manifold (mean = 0 for identifiability)\n",
        "            grad = grad - grad.mean()\n",
        "            \n",
        "            # Check convergence\n",
        "            if np.abs(grad).max() < self.tol:\n",
        "                break\n",
        "            \n",
        "            # Decaying step size for stability\n",
        "            eta = 1.0 / (1 + 0.1 * iteration)\n",
        "            \n",
        "            # Update and re-center\n",
        "            e = e + eta * grad\n",
        "            e = e - e.mean()\n",
        "        \n",
        "        # Scale to interpretable ELO range (centered at 1000, ~200 points = significant diff)\n",
        "        e = e * 200 + 1000\n",
        "        \n",
        "        return e\n",
        "    \n",
        "    def estimate(\n",
        "        self,\n",
        "        doc_scores: np.ndarray,  # Scores from a ranker (proxy for pairwise prefs)\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Estimate ELO scores for documents.\n",
        "        \n",
        "        In a full implementation, we'd use a pairwise model (cross-encoder).\n",
        "        For this demo, we convert point-wise scores to pairwise preferences.\n",
        "        \n",
        "        Args:\n",
        "            doc_scores: Array of scores for each document (higher = more relevant)\n",
        "            \n",
        "        Returns:\n",
        "            Array of ELO scores\n",
        "        \"\"\"\n",
        "        n = len(doc_scores)\n",
        "        \n",
        "        # Build comparison graph\n",
        "        edges = self._build_k_regular_graph(n)\n",
        "        \n",
        "        # Convert point-wise scores to pairwise preferences using Bradley-Terry\n",
        "        # P(i > j) = œÉ(s_i - s_j) where œÉ is sigmoid\n",
        "        preferences = {}\n",
        "        for (i, j) in edges:\n",
        "            score_diff = doc_scores[i] - doc_scores[j]\n",
        "            # Sigmoid with temperature for smoother preferences\n",
        "            preferences[(i, j)] = 1 / (1 + np.exp(-score_diff * 5))\n",
        "        \n",
        "        # Fit Thurstone model\n",
        "        elos = self._fit_thurstone(preferences, edges, n)\n",
        "        \n",
        "        return elos\n",
        "\n",
        "# Test the ELO estimator\n",
        "print(\"üß™ Testing ELO Estimator...\")\n",
        "test_scores = np.array([0.9, 0.7, 0.5, 0.3, 0.1])  # 5 docs with decreasing scores\n",
        "estimator = SparseELOEstimator(comparison_degree=4)\n",
        "test_elos = estimator.estimate(test_scores)\n",
        "print(f\"   Input scores:  {test_scores}\")\n",
        "print(f\"   Output ELOs:   {test_elos.round(0)}\")\n",
        "print(f\"   ‚úÖ ELO order matches score order: {np.all(np.diff(test_elos) < 0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 ELO-Gap Based Selection\n",
        "\n",
        "Once we have ELO scores, we select negatives based on the **gap** from the positive document.\n",
        "\n",
        "**The Goldilocks Principle**: Negatives should be hard enough to provide learning signal, but not so hard they're likely false negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üéöÔ∏è ELO-GAP BASED SELECTOR\n",
        "# ============================================================\n",
        "\n",
        "class ELOGapSelector:\n",
        "    \"\"\"\n",
        "    Selects and weights negatives based on ELO gap from positive.\n",
        "    \n",
        "    The key insight is that \"difficulty\" varies per query. Rank 20 for one\n",
        "    query might be harder than rank 5 for another. ELO gaps normalize this:\n",
        "    - Gap < 100: Too close to positive ‚Üí likely false negative ‚Üí REJECT\n",
        "    - Gap 100-200: Borderline ‚Üí include with reduced weight\n",
        "    - Gap 200-400: Goldilocks zone ‚Üí optimal learning signal\n",
        "    - Gap 400-600: Medium ‚Üí good for early curriculum stages\n",
        "    - Gap > 600: Easy ‚Üí low learning signal\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        danger_zone: float = 100,\n",
        "        goldilocks_zone: Tuple[float, float] = (200, 400),\n",
        "    ):\n",
        "        self.danger_zone = danger_zone\n",
        "        self.goldilocks = goldilocks_zone\n",
        "    \n",
        "    def select_and_weight(\n",
        "        self,\n",
        "        positive_elo: float,\n",
        "        candidate_elos: List[Tuple[int, float]],  # [(idx, elo), ...]\n",
        "        num_negatives: int = 10,\n",
        "        curriculum_tier: int = 4,  # 1=easy only, 4=all difficulty levels\n",
        "    ) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Select negatives and assign weights based on ELO gap.\n",
        "        \n",
        "        Args:\n",
        "            positive_elo: ELO score of the positive document\n",
        "            candidate_elos: List of (doc_idx, elo_score) for candidates\n",
        "            num_negatives: Number of negatives to select\n",
        "            curriculum_tier: Current training phase (1-4)\n",
        "            \n",
        "        Returns:\n",
        "            List of (doc_idx, weight) tuples\n",
        "        \"\"\"\n",
        "        weighted_candidates = []\n",
        "        \n",
        "        for idx, elo in candidate_elos:\n",
        "            gap = positive_elo - elo  # Higher positive ELO = larger gap\n",
        "            \n",
        "            # Assign weight based on gap category\n",
        "            if gap < self.danger_zone:\n",
        "                # Danger zone: too likely to be false negative\n",
        "                weight = 0.0\n",
        "                tier_required = 99  # Never include\n",
        "            elif gap < 200:\n",
        "                # Soft negative zone\n",
        "                weight = 0.5\n",
        "                tier_required = 4  # Only in final curriculum stage\n",
        "            elif gap < self.goldilocks[1]:\n",
        "                # Goldilocks zone: optimal difficulty\n",
        "                weight = 1.0\n",
        "                tier_required = 3\n",
        "            elif gap < 600:\n",
        "                # Medium difficulty\n",
        "                weight = 0.7\n",
        "                tier_required = 2\n",
        "            else:\n",
        "                # Easy negatives\n",
        "                weight = 0.3\n",
        "                tier_required = 1  # Always include\n",
        "            \n",
        "            # Apply curriculum filter\n",
        "            if curriculum_tier >= tier_required and weight > 0:\n",
        "                weighted_candidates.append((idx, weight, gap))\n",
        "        \n",
        "        # Sort by preference: Goldilocks zone first, then by gap\n",
        "        weighted_candidates.sort(key=lambda x: (\n",
        "            0 if self.goldilocks[0] <= x[2] < self.goldilocks[1] else 1,\n",
        "            x[2]\n",
        "        ))\n",
        "        \n",
        "        # Return top-k\n",
        "        return [(idx, weight) for idx, weight, _ in weighted_candidates[:num_negatives]]\n",
        "\n",
        "# Test the selector\n",
        "print(\"üß™ Testing ELO Gap Selector...\")\n",
        "selector = ELOGapSelector()\n",
        "\n",
        "# Simulate: positive has ELO 1200, candidates have various ELOs\n",
        "test_positive_elo = 1200\n",
        "test_candidates = [\n",
        "    (0, 1150),  # Gap 50 ‚Üí Danger zone\n",
        "    (1, 1050),  # Gap 150 ‚Üí Soft\n",
        "    (2, 900),   # Gap 300 ‚Üí Goldilocks ‚úì\n",
        "    (3, 850),   # Gap 350 ‚Üí Goldilocks ‚úì\n",
        "    (4, 700),   # Gap 500 ‚Üí Medium\n",
        "    (5, 400),   # Gap 800 ‚Üí Easy\n",
        "]\n",
        "\n",
        "selected = selector.select_and_weight(test_positive_elo, test_candidates, num_negatives=4)\n",
        "print(f\"   Positive ELO: {test_positive_elo}\")\n",
        "print(f\"   Selected negatives:\")\n",
        "for idx, weight in selected:\n",
        "    gap = test_positive_elo - test_candidates[idx][1]\n",
        "    print(f\"      Doc {idx}: ELO={test_candidates[idx][1]}, Gap={gap}, Weight={weight}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Hybrid Loss Function\n",
        "\n",
        "The ANMI 2.0 loss combines two objectives:\n",
        "\n",
        "1. **Weighted InfoNCE** - Contrastive loss with soft negative weights\n",
        "2. **ELO MSE** - Regression to match predicted scores with ELO targets\n",
        "\n",
        "**Why Hybrid?**\n",
        "- InfoNCE shapes the embedding geometry (alignment + uniformity)\n",
        "- MSE provides calibration and reduces false negative damage\n",
        "- When a false negative has high ELO, MSE gradient opposes InfoNCE gradient ‚Üí damage mitigation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üî• HYBRID LOSS FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "class HybridLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid loss combining Weighted InfoNCE and ELO MSE.\n",
        "    \n",
        "    L = Œ± * L_InfoNCE_weighted + (1-Œ±) * L_MSE\n",
        "    \n",
        "    CRITICAL IMPLEMENTATION NOTES:\n",
        "    1. We use a learnable projection head (elo_head) to map dot products to ELO space.\n",
        "       This avoids gradient scale mismatch between log-scale InfoNCE and squared MSE.\n",
        "    \n",
        "    2. Negative weights are applied INSIDE the softmax denominator, allowing\n",
        "       \"soft\" exclusion of uncertain negatives rather than hard binary decisions.\n",
        "    \n",
        "    Args:\n",
        "        alpha: Mixing coefficient (0=pure MSE, 1=pure InfoNCE)\n",
        "        temperature: Softmax temperature for InfoNCE\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha: float = 0.6,\n",
        "        temperature: float = 0.07,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.tau = temperature\n",
        "        \n",
        "        # Learnable projection from dot products to ELO space\n",
        "        # This is CRITICAL for stable training - avoids gradient mismatch\n",
        "        self.elo_head = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "        )\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        query_emb: torch.Tensor,       # [batch_size, hidden_dim]\n",
        "        positive_emb: torch.Tensor,    # [batch_size, hidden_dim]\n",
        "        negative_embs: torch.Tensor,   # [batch_size, num_neg, hidden_dim]\n",
        "        negative_weights: torch.Tensor, # [batch_size, num_neg]\n",
        "        elo_targets: torch.Tensor,     # [batch_size, 1+num_neg] - target ELO scores\n",
        "    ) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Compute hybrid loss.\n",
        "        \n",
        "        Returns:\n",
        "            total_loss: Scalar loss tensor\n",
        "            metrics: Dict with component losses for logging\n",
        "        \"\"\"\n",
        "        batch_size = query_emb.size(0)\n",
        "        \n",
        "        # === Compute Similarities ===\n",
        "        # Positive: [batch_size]\n",
        "        pos_sim = torch.sum(query_emb * positive_emb, dim=-1)\n",
        "        \n",
        "        # Negative: [batch_size, num_neg]\n",
        "        neg_sim = torch.bmm(\n",
        "            negative_embs,\n",
        "            query_emb.unsqueeze(-1)\n",
        "        ).squeeze(-1)\n",
        "        \n",
        "        # === Weighted InfoNCE Loss ===\n",
        "        # Scale by temperature\n",
        "        pos_sim_scaled = pos_sim / self.tau\n",
        "        neg_sim_scaled = neg_sim / self.tau\n",
        "        \n",
        "        # Apply weights to negatives in the denominator\n",
        "        # Weight=0 effectively removes that negative from the softmax\n",
        "        weighted_neg_exp = negative_weights * torch.exp(neg_sim_scaled)\n",
        "        \n",
        "        # InfoNCE: -log(exp(pos) / (exp(pos) + Œ£ w_i * exp(neg_i)))\n",
        "        denominator = torch.exp(pos_sim_scaled) + weighted_neg_exp.sum(dim=-1)\n",
        "        loss_nce = -pos_sim_scaled + torch.log(denominator + 1e-10)\n",
        "        loss_nce = loss_nce.mean()\n",
        "        \n",
        "        # === MSE Loss with Learnable Projection ===\n",
        "        # Concatenate all similarities: [batch_size, 1+num_neg]\n",
        "        all_sims = torch.cat([pos_sim.unsqueeze(-1), neg_sim], dim=-1)\n",
        "        \n",
        "        # Project to ELO space via learnable head\n",
        "        pred_elo = self.elo_head(all_sims.unsqueeze(-1)).squeeze(-1)\n",
        "        \n",
        "        # MSE loss\n",
        "        loss_mse = F.mse_loss(pred_elo, elo_targets)\n",
        "        \n",
        "        # === Combine ===\n",
        "        total_loss = self.alpha * loss_nce + (1 - self.alpha) * loss_mse\n",
        "        \n",
        "        metrics = {\n",
        "            \"loss\": total_loss.item(),\n",
        "            \"nce\": loss_nce.item(),\n",
        "            \"mse\": loss_mse.item(),\n",
        "        }\n",
        "        \n",
        "        return total_loss, metrics\n",
        "\n",
        "# Test the hybrid loss\n",
        "print(\"üß™ Testing Hybrid Loss...\")\n",
        "loss_fn = HybridLoss(alpha=0.6, temperature=0.07)\n",
        "\n",
        "# Dummy inputs\n",
        "batch_size, hidden_dim, num_neg = 4, 64, 5\n",
        "dummy_query = torch.randn(batch_size, hidden_dim)\n",
        "dummy_pos = torch.randn(batch_size, hidden_dim)\n",
        "dummy_neg = torch.randn(batch_size, num_neg, hidden_dim)\n",
        "dummy_weights = torch.tensor([[1.0, 0.7, 0.5, 0.3, 0.0]] * batch_size)\n",
        "dummy_elos = torch.randn(batch_size, 1 + num_neg) * 200 + 1000\n",
        "\n",
        "loss, metrics = loss_fn(dummy_query, dummy_pos, dummy_neg, dummy_weights, dummy_elos)\n",
        "print(f\"   Total loss: {metrics['loss']:.4f}\")\n",
        "print(f\"   InfoNCE:    {metrics['nce']:.4f}\")\n",
        "print(f\"   MSE:        {metrics['mse']:.4f}\")\n",
        "print(\"   ‚úÖ Loss computation successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Mining Pipeline\n",
        "\n",
        "Now we'll put it all together: retrieve candidates, estimate ELOs, select negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚õèÔ∏è COMPLETE MINING PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "class ANMIMiner:\n",
        "    \"\"\"\n",
        "    Complete ANMI mining pipeline:\n",
        "    1. BM25 retrieval for initial candidates\n",
        "    2. Dense scoring for ELO estimation  \n",
        "    3. ELO-gap based selection with soft weights\n",
        "    \n",
        "    This runs OFFLINE before training (as per the production fix).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        corpus: Dict[str, str],\n",
        "        encoder_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        num_candidates: int = 50,  # BM25 top-k\n",
        "        num_negatives: int = 7,    # Final negatives per query\n",
        "    ):\n",
        "        self.corpus = corpus\n",
        "        self.corpus_ids = list(corpus.keys())\n",
        "        self.corpus_texts = list(corpus.values())\n",
        "        self.num_candidates = num_candidates\n",
        "        self.num_negatives = num_negatives\n",
        "        \n",
        "        print(\"üîß Initializing ANMI Miner...\")\n",
        "        \n",
        "        # Initialize BM25 index\n",
        "        print(\"   Building BM25 index...\")\n",
        "        tokenized_corpus = [doc.lower().split() for doc in self.corpus_texts]\n",
        "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "        \n",
        "        # Initialize encoder for scoring\n",
        "        print(f\"   Loading encoder: {encoder_model}\")\n",
        "        self.encoder = SentenceTransformer(encoder_model, device=DEVICE)\n",
        "        \n",
        "        # Pre-encode corpus (this takes a minute)\n",
        "        print(\"   Encoding corpus (this may take a minute)...\")\n",
        "        self.corpus_embeddings = self.encoder.encode(\n",
        "            self.corpus_texts,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=True,\n",
        "            batch_size=64,\n",
        "        )\n",
        "        \n",
        "        # Initialize components\n",
        "        self.elo_estimator = SparseELOEstimator(comparison_degree=4)\n",
        "        self.selector = ELOGapSelector()\n",
        "        \n",
        "        print(\"   ‚úÖ Miner ready!\")\n",
        "    \n",
        "    def mine_for_query(\n",
        "        self,\n",
        "        query: str,\n",
        "        positive_ids: List[str],\n",
        "        curriculum_tier: int = 4,\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Mine negatives for a single query.\n",
        "        \n",
        "        Returns dict with:\n",
        "            - query: str\n",
        "            - positive: str (first positive doc)\n",
        "            - positive_elo: float\n",
        "            - negatives: List[str]\n",
        "            - negative_weights: List[float]\n",
        "            - negative_elos: List[float]\n",
        "        \"\"\"\n",
        "        # Step 1: BM25 retrieval for candidates\n",
        "        query_tokens = query.lower().split()\n",
        "        bm25_scores = self.bm25.get_scores(query_tokens)\n",
        "        top_indices = np.argsort(bm25_scores)[::-1][:self.num_candidates]\n",
        "        \n",
        "        candidate_ids = [self.corpus_ids[i] for i in top_indices]\n",
        "        candidate_texts = [self.corpus_texts[i] for i in top_indices]\n",
        "        \n",
        "        # Remove positives from candidates\n",
        "        positive_set = set(positive_ids)\n",
        "        filtered = [(cid, ctxt, i) for i, (cid, ctxt) in enumerate(zip(candidate_ids, candidate_texts)) \n",
        "                    if cid not in positive_set]\n",
        "        \n",
        "        if len(filtered) < 3:\n",
        "            # Not enough negatives, return None\n",
        "            return None\n",
        "        \n",
        "        candidate_ids = [x[0] for x in filtered]\n",
        "        candidate_texts = [x[1] for x in filtered]\n",
        "        original_indices = [x[2] for x in filtered]\n",
        "        \n",
        "        # Step 2: Dense scoring for ELO estimation\n",
        "        query_emb = self.encoder.encode(query, convert_to_tensor=True)\n",
        "        candidate_embs = self.corpus_embeddings[[self.corpus_ids.index(cid) for cid in candidate_ids]]\n",
        "        \n",
        "        # Compute similarities\n",
        "        sims = torch.nn.functional.cosine_similarity(\n",
        "            query_emb.unsqueeze(0), candidate_embs\n",
        "        ).cpu().numpy()\n",
        "        \n",
        "        # Get positive embedding and similarity\n",
        "        pos_id = positive_ids[0]\n",
        "        pos_text = self.corpus[pos_id]\n",
        "        pos_idx = self.corpus_ids.index(pos_id)\n",
        "        pos_sim = torch.nn.functional.cosine_similarity(\n",
        "            query_emb.unsqueeze(0), \n",
        "            self.corpus_embeddings[pos_idx].unsqueeze(0)\n",
        "        ).item()\n",
        "        \n",
        "        # Step 3: ELO estimation\n",
        "        all_sims = np.concatenate([[pos_sim], sims])\n",
        "        elos = self.elo_estimator.estimate(all_sims)\n",
        "        \n",
        "        pos_elo = elos[0]\n",
        "        candidate_elos = [(i, elos[i+1]) for i in range(len(candidate_ids))]\n",
        "        \n",
        "        # Step 4: ELO-gap selection\n",
        "        selected = self.selector.select_and_weight(\n",
        "            positive_elo=pos_elo,\n",
        "            candidate_elos=candidate_elos,\n",
        "            num_negatives=self.num_negatives,\n",
        "            curriculum_tier=curriculum_tier,\n",
        "        )\n",
        "        \n",
        "        if len(selected) == 0:\n",
        "            return None\n",
        "        \n",
        "        # Build result\n",
        "        neg_texts = [candidate_texts[idx] for idx, _ in selected]\n",
        "        neg_weights = [weight for _, weight in selected]\n",
        "        neg_elos = [elos[idx + 1] for idx, _ in selected]\n",
        "        \n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"positive\": pos_text,\n",
        "            \"positive_elo\": pos_elo,\n",
        "            \"negatives\": neg_texts,\n",
        "            \"negative_weights\": neg_weights,\n",
        "            \"negative_elos\": neg_elos,\n",
        "        }\n",
        "    \n",
        "    def mine_dataset(\n",
        "        self,\n",
        "        queries: Dict[str, str],\n",
        "        positives: Dict[str, List[str]],\n",
        "        query_ids: List[str],\n",
        "        curriculum_tier: int = 4,\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Mine negatives for all queries.\n",
        "        \"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        for qid in tqdm(query_ids, desc=f\"Mining (tier={curriculum_tier})\"):\n",
        "            if qid not in queries or qid not in positives:\n",
        "                continue\n",
        "            \n",
        "            result = self.mine_for_query(\n",
        "                query=queries[qid],\n",
        "                positive_ids=positives[qid],\n",
        "                curriculum_tier=curriculum_tier,\n",
        "            )\n",
        "            \n",
        "            if result is not None:\n",
        "                examples.append(result)\n",
        "        \n",
        "        return examples\n",
        "\n",
        "# Initialize the miner\n",
        "print(\"üöÄ Initializing ANMI Miner...\")\n",
        "miner = ANMIMiner(\n",
        "    corpus=corpus,\n",
        "    encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    num_candidates=50,\n",
        "    num_negatives=7,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üì¶ OFFLINE MINING (Run ONCE before training)\n",
        "# ============================================================\n",
        "# This is the expensive step - we pre-compute everything so training is fast\n",
        "\n",
        "print(\"‚õèÔ∏è Running OFFLINE mining...\")\n",
        "print(\"   This pre-computes negatives, ELOs, and weights.\")\n",
        "print(\"   Training will then be fast (just forward/backward passes).\\n\")\n",
        "\n",
        "# Mine for all curriculum tiers\n",
        "mined_data = miner.mine_dataset(\n",
        "    queries=queries,\n",
        "    positives=positives,\n",
        "    query_ids=query_ids[:100],  # Use subset for Colab speed\n",
        "    curriculum_tier=4,  # All difficulty levels\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Mined {len(mined_data)} training examples\")\n",
        "\n",
        "# Show statistics\n",
        "avg_weight = np.mean([np.mean(ex[\"negative_weights\"]) for ex in mined_data])\n",
        "avg_neg = np.mean([len(ex[\"negatives\"]) for ex in mined_data])\n",
        "print(f\"   Average negatives per query: {avg_neg:.1f}\")\n",
        "print(f\"   Average negative weight: {avg_weight:.2f}\")\n",
        "\n",
        "# Show one example\n",
        "print(f\"\\nüìù Example mined data:\")\n",
        "ex = mined_data[0]\n",
        "print(f\"   Query: {ex['query'][:80]}...\")\n",
        "print(f\"   Positive ELO: {ex['positive_elo']:.0f}\")\n",
        "print(f\"   Negatives: {len(ex['negatives'])}\")\n",
        "for i, (w, e) in enumerate(zip(ex[\"negative_weights\"][:3], ex[\"negative_elos\"][:3])):\n",
        "    gap = ex[\"positive_elo\"] - e\n",
        "    print(f\"      Neg {i}: ELO={e:.0f}, Gap={gap:.0f}, Weight={w:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training\n",
        "\n",
        "Now we train the model using our pre-mined data with the hybrid loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üìä DATASET CLASS\n",
        "# ============================================================\n",
        "\n",
        "class ANMIDataset(TorchDataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for ANMI training.\n",
        "    \n",
        "    Handles padding negatives to fixed size and converting to tensors.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, mined_data: List[Dict], encoder: SentenceTransformer, num_negatives: int = 7):\n",
        "        self.data = mined_data\n",
        "        self.encoder = encoder\n",
        "        self.num_negatives = num_negatives\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        \n",
        "        # Pad negatives to fixed size if needed\n",
        "        negatives = ex[\"negatives\"][:self.num_negatives]\n",
        "        weights = ex[\"negative_weights\"][:self.num_negatives]\n",
        "        elos = ex[\"negative_elos\"][:self.num_negatives]\n",
        "        \n",
        "        # Pad if not enough negatives\n",
        "        while len(negatives) < self.num_negatives:\n",
        "            negatives.append(\"\")  # Empty string for padding\n",
        "            weights.append(0.0)   # Zero weight (ignored in loss)\n",
        "            elos.append(500.0)    # Placeholder ELO\n",
        "        \n",
        "        return {\n",
        "            \"query\": ex[\"query\"],\n",
        "            \"positive\": ex[\"positive\"],\n",
        "            \"positive_elo\": ex[\"positive_elo\"],\n",
        "            \"negatives\": negatives,\n",
        "            \"negative_weights\": torch.tensor(weights, dtype=torch.float32),\n",
        "            \"negative_elos\": torch.tensor(elos, dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for batching.\"\"\"\n",
        "    return {\n",
        "        \"queries\": [ex[\"query\"] for ex in batch],\n",
        "        \"positives\": [ex[\"positive\"] for ex in batch],\n",
        "        \"positive_elos\": torch.stack([torch.tensor(ex[\"positive_elo\"]) for ex in batch]),\n",
        "        \"negatives\": [ex[\"negatives\"] for ex in batch],  # List of lists\n",
        "        \"negative_weights\": torch.stack([ex[\"negative_weights\"] for ex in batch]),\n",
        "        \"negative_elos\": torch.stack([ex[\"negative_elos\"] for ex in batch]),\n",
        "    }\n",
        "\n",
        "# Create dataset\n",
        "dataset = ANMIDataset(mined_data, miner.encoder, num_negatives=7)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"‚úÖ Created dataset with {len(dataset)} examples\")\n",
        "print(f\"   Batch size: 8\")\n",
        "print(f\"   Batches per epoch: {len(dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üèãÔ∏è TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "def train_anmi(\n",
        "    encoder: SentenceTransformer,\n",
        "    dataloader: DataLoader,\n",
        "    num_epochs: int = 3,\n",
        "    learning_rate: float = 2e-5,\n",
        "    alpha: float = 0.6,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train encoder with ANMI hybrid loss.\n",
        "    \n",
        "    Key features:\n",
        "    - Pre-mined negatives with ELO-calibrated weights (OFFLINE)\n",
        "    - Hybrid loss: Œ± * InfoNCE + (1-Œ±) * MSE\n",
        "    - Curriculum: could progressively increase alpha\n",
        "    \"\"\"\n",
        "    # Initialize loss and optimizer\n",
        "    loss_fn = HybridLoss(alpha=alpha, temperature=0.07).to(DEVICE)\n",
        "    \n",
        "    # Combine encoder and loss head parameters\n",
        "    all_params = list(encoder.parameters()) + list(loss_fn.parameters())\n",
        "    optimizer = torch.optim.AdamW(all_params, lr=learning_rate)\n",
        "    \n",
        "    encoder.train()\n",
        "    history = []\n",
        "    \n",
        "    print(f\"üèãÔ∏è Training for {num_epochs} epochs...\")\n",
        "    print(f\"   Learning rate: {learning_rate}\")\n",
        "    print(f\"   Alpha (NCE weight): {alpha}\")\n",
        "    print()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {\"loss\": [], \"nce\": [], \"mse\": []}\n",
        "        \n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Encode queries\n",
        "            query_embs = encoder.encode(\n",
        "                batch[\"queries\"],\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False,\n",
        "            ).to(DEVICE)\n",
        "            \n",
        "            # Encode positives\n",
        "            pos_embs = encoder.encode(\n",
        "                batch[\"positives\"],\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False,\n",
        "            ).to(DEVICE)\n",
        "            \n",
        "            # Encode negatives (flatten, encode, reshape)\n",
        "            batch_size = len(batch[\"queries\"])\n",
        "            num_neg = len(batch[\"negatives\"][0])\n",
        "            \n",
        "            flat_negatives = [neg for negs in batch[\"negatives\"] for neg in negs]\n",
        "            neg_embs = encoder.encode(\n",
        "                flat_negatives,\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False,\n",
        "            ).to(DEVICE)\n",
        "            neg_embs = neg_embs.view(batch_size, num_neg, -1)\n",
        "            \n",
        "            # Get weights and ELOs\n",
        "            weights = batch[\"negative_weights\"].to(DEVICE)\n",
        "            pos_elos = batch[\"positive_elos\"].to(DEVICE)\n",
        "            neg_elos = batch[\"negative_elos\"].to(DEVICE)\n",
        "            \n",
        "            # Combine ELOs: [batch_size, 1+num_neg]\n",
        "            all_elos = torch.cat([pos_elos.unsqueeze(-1), neg_elos], dim=-1)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss, metrics = loss_fn(\n",
        "                query_emb=query_embs,\n",
        "                positive_emb=pos_embs,\n",
        "                negative_embs=neg_embs,\n",
        "                negative_weights=weights,\n",
        "                elo_targets=all_elos,\n",
        "            )\n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(all_params, 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track\n",
        "            epoch_losses[\"loss\"].append(metrics[\"loss\"])\n",
        "            epoch_losses[\"nce\"].append(metrics[\"nce\"])\n",
        "            epoch_losses[\"mse\"].append(metrics[\"mse\"])\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{metrics['loss']:.3f}\",\n",
        "                \"nce\": f\"{metrics['nce']:.3f}\",\n",
        "                \"mse\": f\"{metrics['mse']:.3f}\",\n",
        "            })\n",
        "        \n",
        "        # Epoch summary\n",
        "        avg_loss = np.mean(epoch_losses[\"loss\"])\n",
        "        avg_nce = np.mean(epoch_losses[\"nce\"])\n",
        "        avg_mse = np.mean(epoch_losses[\"mse\"])\n",
        "        \n",
        "        history.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"loss\": avg_loss,\n",
        "            \"nce\": avg_nce,\n",
        "            \"mse\": avg_mse,\n",
        "        })\n",
        "        \n",
        "        print(f\"   Epoch {epoch+1}: Loss={avg_loss:.4f}, NCE={avg_nce:.4f}, MSE={avg_mse:.4f}\")\n",
        "    \n",
        "    return encoder, history\n",
        "\n",
        "# Train the model\n",
        "print(\"üöÄ Starting ANMI Training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trained_encoder, training_history = train_anmi(\n",
        "    encoder=miner.encoder,\n",
        "    dataloader=dataloader,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    alpha=0.6,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation\n",
        "\n",
        "Let's evaluate the trained model and compare with baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üìà EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_retrieval(\n",
        "    encoder: SentenceTransformer,\n",
        "    queries: Dict[str, str],\n",
        "    corpus: Dict[str, str],\n",
        "    positives: Dict[str, List[str]],\n",
        "    query_ids: List[str],\n",
        "    top_k: int = 10,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate retrieval performance.\n",
        "    \n",
        "    Metrics:\n",
        "    - MRR@k: Mean Reciprocal Rank\n",
        "    - Recall@k: Fraction of positives in top-k\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    \n",
        "    corpus_ids = list(corpus.keys())\n",
        "    corpus_texts = list(corpus.values())\n",
        "    \n",
        "    # Encode corpus\n",
        "    print(\"   Encoding corpus for evaluation...\")\n",
        "    corpus_embs = encoder.encode(\n",
        "        corpus_texts,\n",
        "        convert_to_tensor=True,\n",
        "        show_progress_bar=True,\n",
        "        batch_size=64,\n",
        "    )\n",
        "    \n",
        "    mrr_scores = []\n",
        "    recall_scores = []\n",
        "    \n",
        "    print(\"   Computing metrics...\")\n",
        "    for qid in tqdm(query_ids, desc=\"Evaluating\"):\n",
        "        if qid not in queries or qid not in positives:\n",
        "            continue\n",
        "        \n",
        "        query_text = queries[qid]\n",
        "        pos_ids = set(positives[qid])\n",
        "        \n",
        "        # Encode query\n",
        "        query_emb = encoder.encode(query_text, convert_to_tensor=True)\n",
        "        \n",
        "        # Compute similarities\n",
        "        sims = torch.nn.functional.cosine_similarity(\n",
        "            query_emb.unsqueeze(0), corpus_embs\n",
        "        )\n",
        "        \n",
        "        # Get top-k\n",
        "        top_indices = torch.argsort(sims, descending=True)[:top_k].cpu().numpy()\n",
        "        top_ids = [corpus_ids[i] for i in top_indices]\n",
        "        \n",
        "        # MRR: Reciprocal of first positive's rank\n",
        "        mrr = 0.0\n",
        "        for rank, doc_id in enumerate(top_ids, 1):\n",
        "            if doc_id in pos_ids:\n",
        "                mrr = 1.0 / rank\n",
        "                break\n",
        "        mrr_scores.append(mrr)\n",
        "        \n",
        "        # Recall: Fraction of positives in top-k\n",
        "        hits = len(pos_ids.intersection(set(top_ids)))\n",
        "        recall = hits / len(pos_ids) if pos_ids else 0.0\n",
        "        recall_scores.append(recall)\n",
        "    \n",
        "    return {\n",
        "        f\"MRR@{top_k}\": np.mean(mrr_scores),\n",
        "        f\"Recall@{top_k}\": np.mean(recall_scores),\n",
        "    }\n",
        "\n",
        "# Evaluate on test queries (different from training)\n",
        "test_query_ids = query_ids[100:150]  # Use queries we didn't train on\n",
        "\n",
        "print(\"üìä Evaluating ANMI-trained model...\")\n",
        "anmi_metrics = evaluate_retrieval(\n",
        "    encoder=trained_encoder,\n",
        "    queries=queries,\n",
        "    corpus=corpus,\n",
        "    positives=positives,\n",
        "    query_ids=test_query_ids,\n",
        "    top_k=10,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ ANMI Results:\")\n",
        "for metric, value in anmi_metrics.items():\n",
        "    print(f\"   {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "### What We Implemented\n",
        "\n",
        "1. **Sparse ELO Estimator** - Converts O(n) pairwise comparisons to absolute quality scores using Thurstone MLE on k-regular graphs\n",
        "\n",
        "2. **ELO-Gap Selector** - Selects negatives based on quality gap, not rank, with soft continuous weights\n",
        "\n",
        "3. **Hybrid Loss** - Combines InfoNCE (geometry) + MSE (calibration) with learnable projection head\n",
        "\n",
        "4. **Offline Mining Pipeline** - Pre-computes everything expensive so training is fast\n",
        "\n",
        "### Key Innovations\n",
        "\n",
        "| Component | Traditional | ANMI 2.0 |\n",
        "|-----------|-------------|----------|\n",
        "| Negative Selection | Rank-based | ELO gap-based |\n",
        "| Negative Weighting | Binary (include/exclude) | Soft continuous |\n",
        "| Loss Function | Pure InfoNCE | Hybrid (InfoNCE + MSE) |\n",
        "| False Negative Handling | Ad-hoc threshold | ELO danger zone |\n",
        "| Computation | Online (slow) | Offline mining (fast training) |\n",
        "\n",
        "### Production Considerations Addressed\n",
        "\n",
        "1. **Learnable ELO head** - Fixes gradient scale mismatch between InfoNCE and MSE\n",
        "2. **Offline mining** - Avoids 10-50x slowdown from online validation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try curriculum learning (progressive difficulty)\n",
        "- Compare with baselines on larger datasets\n",
        "- Implement full cross-encoder pairwise model for ELO estimation\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "- ANMI 2.0 Paper (forthcoming)\n",
        "- [Tevatron](https://github.com/texttron/tevatron) - Dense retrieval toolkit\n",
        "- [Dense Text Retrieval Survey](https://arxiv.org/abs/2211.14876) - Zhao et al.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
